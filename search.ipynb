{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sementic Search Summary\n",
    "\n",
    "## Setting up Semantic Search from Scratch\n",
    "\n",
    "This tutorial will guide you through the entire process of setting up the semantic search project from scratch, including installing Python, creating a virtual environment, installing the necessary libraries, and running the code.\n",
    "\n",
    "### Step 1: Install Python\n",
    "\n",
    "First, you need to have Python installed on your system. Download and install the latest version of Python from the official website:\n",
    "\n",
    "[https://www.python.org/downloads/](https://www.python.org/downloads/)\n",
    "\n",
    "### Step 2: Install pip (if not installed)\n",
    "\n",
    "Pip is the package installer for Python. It is usually included with the Python installation. However, if it's not installed, you can download and install it by following the instructions here:\n",
    "\n",
    "[https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/)\n",
    "\n",
    "### Step 3: Create a virtual environment (Optional)\n",
    "\n",
    "Although not required, it's a good practice to create a virtual environment to keep your project dependencies separate from your system-wide Python installation.\n",
    "\n",
    "Create a new directory for your project:\n",
    "\n",
    "```\n",
    "mkdir semantic_search\n",
    "cd semantic_search\n",
    "```\n",
    "\n",
    "Create a virtual environment using `venv`:\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "\n",
    "Activate the virtual environment:\n",
    "\n",
    "- On Windows:\n",
    "\n",
    "```\n",
    "venv\\Scripts\\activate\n",
    "```\n",
    "- On macOS and Linux:\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### Step 4: Install dependencies\n",
    "\n",
    "Install the required libraries using `pip`:\n",
    "\n",
    "```\n",
    "pip install pandas numpy sklearn transformers torch nltk\n",
    "```\n",
    "\n",
    "\n",
    "## Cell 1: Import libraries\n",
    "\n",
    "This cell imports the necessary libraries for the script:\n",
    "\n",
    "- **pandas**: A powerful data manipulation library that allows us to work with DataFrames, which are essentially tables of data. We use pandas to read the input CSV file and store the data in a structured format.\n",
    "- **numpy**: A library for working with arrays and numerical operations. We use numpy to work with embeddings and compute similarities between them.\n",
    "- **sklearn.metrics.pairwise**: A submodule from the scikit-learn library that provides the cosine_similarity function. We use this function to calculate cosine similarities between the embeddings of summaries and the query.\n",
    "- **transformers**: A library that provides pre-trained natural language processing models and tools. We use transformers to load a pre-trained sentence transformer model and its tokenizer to generate embeddings for the text.\n",
    "- **torch**: A library for working with deep learning models. We use torch to handle the computation of embeddings using the transformer model.\n",
    "- **ast**: The Abstract Syntax Tree library is used to convert strings containing list representations back into actual lists. We use ast to convert the saved embeddings back into numpy arrays when loading the data from the CSV file.\n",
    "- **nltk**: The Natural Language Toolkit is a library for working with human language data (text). We use nltk for text preprocessing, including tokenizing, removing stopwords, and lemmatizing.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 2: Downloading NLTK resources\n",
    "\n",
    "This cell downloads the necessary resources from the NLTK library for text preprocessing:\n",
    "\n",
    "- **punkt**: The 'punkt' resource is a pre-trained tokenizer for several languages. It is used by the `word_tokenize` function, which is responsible for splitting a text into individual words or tokens. In this script, we use 'punkt' to tokenize text during the preprocessing step.\n",
    "\n",
    "- **stopwords**: Stopwords are common words that usually do not provide valuable information for text analysis tasks. Examples of stopwords in English include 'a', 'an', 'the', 'in', 'and', etc. The 'stopwords' resource provides a collection of such words for various languages. In this script, we use the 'stopwords' resource to filter out stopwords from the text during the preprocessing step.\n",
    "\n",
    "- **wordnet**: WordNet is a large lexical database of English. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records various semantic relations between these synonym sets. The 'wordnet' resource is used by the `WordNetLemmatizer` class in NLTK, which is responsible for converting words to their base form or lemma. In this script, we use the 'wordnet' resource to lemmatize words during the preprocessing step.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 3: Define necessary functions\n",
    "\n",
    "1. `preprocess_text(text)`: This function takes a text input and performs the following preprocessing steps:\n",
    "\n",
    "    - Converts the text to lowercase.\n",
    "    - Tokenizes the text into individual words.\n",
    "    - Removes stopwords (common words that don't carry much meaning, like \"the\", \"and\", etc.).\n",
    "    - Lemmatizes the tokens (reduces words to their base form, e.g., \"running\" becomes \"run\").\n",
    "    - Joins the processed tokens back together into a single string.\n",
    "\n",
    "2. `generate_embedding(text)`: This function takes a text input and generates an embedding for the text using the pre-trained transformer model. It processes the input text with the tokenizer and feeds it into the model to obtain the last hidden state. It then computes the mean of the last hidden state along dimension 1 to generate the final embedding.\n",
    "\n",
    "3. `extract_relevant_sentences(summary, query_embedding, k=2)`: This function takes a summary, a query embedding, and an optional parameter k (default value 2). It extracts the top-k most relevant sentences from the summary based on cosine similarity with the query embedding. The function tokenizes the summary into sentences, generates sentence embeddings, computes the cosine similarities between the query embedding and sentence embeddings, and finally selects the top-k sentences with the highest similarities.\n",
    "\n",
    "4. `search_authors_and_relevant_parts(df, query, n=3, k=2)`: This function takes a DataFrame, a query, and optional parameters n (default value 3) and k (default value 2). It searches for the top-n authors with the most relevant parts of their summaries based on the input query. The function generates a query embedding, computes the cosine similarities between the query embedding and summary embeddings in the DataFrame, sorts the authors based on similarity, selects the top-n authors, and extracts the top-k most relevant sentences from their summaries.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def extract_relevant_sentences(summary, query_embedding, k=2):\n",
    "    sentences = summary.split(\".\")\n",
    "    sentence_embeddings = [generate_embedding(sentence) for sentence in sentences]\n",
    "    similarities = [cosine_similarity(query_embedding.reshape(1, -1), sentence_embedding.reshape(1, -1))[0, 0] for sentence_embedding in sentence_embeddings]\n",
    "    top_k_indices = np.argsort(similarities)[-k:]\n",
    "    return [sentences[index] for index in top_k_indices]\n",
    "\n",
    "def search_authors_and_relevant_parts(df, query, n=3, k=2):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    df[\"similarity\"] = df[\"embedding\"].apply(lambda x: cosine_similarity(x.reshape(1, -1), query_embedding.reshape(1, -1))[0, 0])\n",
    "\n",
    "    top_n_results = df.sort_values(\"similarity\", ascending=False).head(n)\n",
    "\n",
    "    for _, row in top_n_results.iterrows():\n",
    "        relevant_parts = extract_relevant_sentences(row[\"summary\"], query_embedding, k)\n",
    "        print(f\"Author: {row['author']} (similarity: {row['similarity']:.4f})\")\n",
    "        print(f\"Relevant parts: {'.'.join(relevant_parts)}...\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 4: Load model and tokenizer\n",
    "In this cell, we load the pre-trained model and tokenizer using the specified model name. The pre-trained model is based on a transformer architecture, which has become the go-to architecture for many natural language processing tasks due to its ability to effectively capture contextual information in text.\n",
    "\n",
    "Transformers are a type of deep learning model that use self-attention mechanisms to weigh the importance of words in a sequence. This helps them to understand the context and relationships between words in a sentence or a text. In this tutorial, we will use the transformer model to generate embeddings for text, which can then be used to perform semantic search.\n",
    "\n",
    "Model names in the Hugging Face Model Hub are unique identifiers for pre-trained models. In this case, we have chosen the \"sentence-transformers/paraphrase-MiniLM-L6-v2\" model. This model is a smaller and faster variant of the more powerful BERT models, designed specifically for the task of generating paraphrase-related sentence embeddings.\n",
    "\n",
    "A tokenizer is a necessary component for working with transformer models. It is responsible for converting raw text into a format that can be fed into the model, including tokenization, mapping tokens to IDs, and creating input tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "datafile_path = \"summaries.csv\"\n",
    "output_file_path = \"summaries_with_embeddings.csv\"\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 5: Generate embeddings and save to a cell\n",
    "In this cell, we generate embeddings for the preprocessed summaries and save them to a separate CSV file. The main rationale behind generating and saving embeddings to a separate CSV file are:\n",
    "\n",
    "1. **Computational cost**: Generating embeddings using transformer models can be computationally expensive, especially when dealing with a large number of texts. By generating the embeddings once and saving them to a separate file, we can avoid the need to recompute them each time we want to perform a search. This significantly reduces the computational overhead and speeds up the search process.\n",
    "\n",
    "2. **Reusability**: Storing the embeddings in a separate CSV file allows us to reuse them in future searches or other tasks that require text similarity comparisons. This way, we can save time and resources by not having to recompute the embeddings each time they are needed.\n",
    "\n",
    "3. **Portability**: Saving the embeddings to a file makes it easier to share and transfer the data between different environments or systems. This can be particularly useful when working with remote servers or collaborating with others.\n",
    "\n",
    "By generating and saving embeddings to a separate CSV file, we can optimize the search process and make it more efficient, especially when working with large datasets or computationally expensive models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Generate embeddings and save to a cell\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"preprocessed_summary\"] = df.summary.apply(preprocess_text)\n",
    "df[\"embedding\"] = df.preprocessed_summary.apply(generate_embedding).apply(lambda x: ', '.join(map(str, x[0])))\n",
    "df.to_csv(output_file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Cell 6: Load embeddings and define a search function\n",
    "\n",
    "In this cell, we define a `search` function that takes a query as input and finds the most relevant authors and parts of their summaries. The search function calls `search_authors_and_relevant_parts` and that works as follows:\n",
    "\n",
    "1. First, it generates an embedding for the query using the same pre-trained transformer model as used for the summaries. This ensures that the query embedding and summary embeddings are in the same semantic space.\n",
    "\n",
    "2. Next, the function computes the cosine similarities between the query embedding and each of the summary embeddings. Cosine similarity is a widely used metric to measure the similarity between two vectors, in our case, the query and summary embeddings. A higher cosine similarity indicates a higher degree of similarity between the query and the summary.\n",
    "\n",
    "3. The function then sorts the authors based on their similarity scores and selects the top-n authors with the highest cosine similarities. This provides us with the most relevant authors based on the input query.\n",
    "\n",
    "4. Finally, the function extracts the top-k most relevant sentences from each of the selected author's summaries. This is done by computing the cosine similarities between the query embedding and individual sentence embeddings within each summary, and selecting the sentences with the highest similarities.\n",
    "\n",
    "By following these steps, the search function provides a ranked list of authors along with the most relevant parts of their summaries, based on the input query.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Load embeddings and define a search function\n",
    "df_with_embeddings = pd.read_csv(output_file_path)\n",
    "df_with_embeddings[\"embedding\"] = df_with_embeddings.embedding.apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "def search(query):\n",
    "    search_authors_and_relevant_parts(df_with_embeddings, query, n=5, k=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 7: Test the search function\n",
    "This cell runs the `search` function with an example query (\"Plant biology\") to see how it works and to display the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Garcia, M. (similarity: 0.3946)\n",
      "Relevant parts:  Four types of growth regulators, including auxin, cytokinin, gibberellin, and abscisic acid, were applied to tomato plants under controlled conditions.The objective of this project was to investigate the effects of different plant growth regulators on the growth and development of tomato plants...\n",
      "Author: Lee, H. (similarity: 0.2827)\n",
      "Relevant parts:  The plants were grown for a period of six weeks, and growth parameters such as shoot and root length, leaf area, and dry weight were measured at regular intervals.The objective of this project was to investigate the effects of different light spectra on the growth and nutritional content of lettuce plants...\n",
      "Author: Johnson, A. (similarity: 0.2698)\n",
      "Relevant parts: The objective of this project was to investigate the effect of different concentrations of caffeine on the growth rate of Arabidopsis thaliana. These findings suggest that caffeine may have a negative impact on the growth of Arabidopsis thaliana and could potentially affect the growth of other plant species...\n",
      "Author: Smith, J. (similarity: 0.2602)\n",
      "Relevant parts:  The plants were grown under controlled conditions for a period of 10 weeks, and the growth parameters such as plant height, stem diameter, and number of leaves were measured at regular intervals.The aim of this project was to investigate the effect of different types of soil on the growth of tomato plants...\n",
      "Author: Kim, S. (similarity: 0.2473)\n",
      "Relevant parts:  These findings suggest that moderate irrigation is optimal for the growth and yield of strawberry plants, and excessive irrigation can have negative effects on plant growth and productivity.The objective of this project was to investigate the effects of different irrigation regimes on the growth and yield of strawberry plants...\n"
     ]
    }
   ],
   "source": [
    "# Test the search function\n",
    "\n",
    "search(\"plant molecular biology\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
