{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sementic Search Summary\n",
    "\n",
    "## Setting up Semantic Search from Scratch\n",
    "\n",
    "This tutorial will guide you through the entire process of setting up the semantic search project from scratch, including installing Python, creating a virtual environment, installing the necessary libraries, and running the code.\n",
    "\n",
    "### Step 1: Install Python\n",
    "\n",
    "First, you need to have Python installed on your system. Download and install the latest version of Python from the official website:\n",
    "\n",
    "[https://www.python.org/downloads/](https://www.python.org/downloads/)\n",
    "\n",
    "### Step 2: Install pip (if not installed)\n",
    "\n",
    "Pip is the package installer for Python. It is usually included with the Python installation. However, if it's not installed, you can download and install it by following the instructions here:\n",
    "\n",
    "[https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/)\n",
    "\n",
    "### Step 3: Create a virtual environment (Optional)\n",
    "\n",
    "Although not required, it's a good practice to create a virtual environment to keep your project dependencies separate from your system-wide Python installation.\n",
    "\n",
    "Create a new directory for your project:\n",
    "\n",
    "```\n",
    "mkdir semantic_search\n",
    "cd semantic_search\n",
    "```\n",
    "\n",
    "Create a virtual environment using `venv`:\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "\n",
    "Activate the virtual environment:\n",
    "\n",
    "- On Windows:\n",
    "\n",
    "```\n",
    "venv\\Scripts\\activate\n",
    "```\n",
    "- On macOS and Linux:\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "### Step 4: Install dependencies\n",
    "\n",
    "Install the required libraries using `pip`:\n",
    "\n",
    "```\n",
    "pip install pandas numpy sklearn transformers torch nltk\n",
    "```\n",
    "\n",
    "\n",
    "## Cell 1: Import libraries\n",
    "This cell imports the necessary libraries for data manipulation (pandas, numpy), text preprocessing (nltk), cosine similarity calculation (sklearn), and transformer models (transformers).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 2: Downloading required NLTK resources\n",
    "Here, we download the necessary resources ('punkt', 'stopwords', 'wordnet') for text preprocessing using the NLTK library.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jorrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 3: Define necessary functions\n",
    "This cell contains the definitions of four functions:\n",
    "1. `preprocess_text`: Preprocesses text by converting to lowercase, tokenizing, removing stopwords, and lemmatizing.\n",
    "2. `generate_embedding`: Generates embeddings for a given text using the pre-trained model.\n",
    "3. `extract_relevant_sentences`: Extracts the top-k most relevant sentences from a summary based on cosine similarity with a given query embedding.\n",
    "4. `search_authors_and_relevant_parts`: Searches for the top-n authors with the most relevant parts of their summaries based on a query."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def extract_relevant_sentences(summary, query_embedding, k=2):\n",
    "    sentences = summary.split(\".\")\n",
    "    sentence_embeddings = [generate_embedding(sentence) for sentence in sentences]\n",
    "    similarities = [cosine_similarity(query_embedding.reshape(1, -1), sentence_embedding.reshape(1, -1))[0, 0] for sentence_embedding in sentence_embeddings]\n",
    "    top_k_indices = np.argsort(similarities)[-k:]\n",
    "    return [sentences[index] for index in top_k_indices]\n",
    "\n",
    "def search_authors_and_relevant_parts(df, query, n=3, k=2):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    df[\"similarity\"] = df[\"embedding\"].apply(lambda x: cosine_similarity(x.reshape(1, -1), query_embedding.reshape(1, -1))[0, 0])\n",
    "\n",
    "    top_n_results = df.sort_values(\"similarity\", ascending=False).head(n)\n",
    "\n",
    "    for _, row in top_n_results.iterrows():\n",
    "        relevant_parts = extract_relevant_sentences(row[\"summary\"], query_embedding, k)\n",
    "        print(f\"Author: {row['author']} (similarity: {row['similarity']:.4f})\")\n",
    "        print(f\"Relevant parts: {'.'.join(relevant_parts)}...\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 4: Load model and tokenizer\n",
    "This cell loads the pre-trained model and tokenizer using the specified model name."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "datafile_path = \"summaries.csv\"\n",
    "output_file_path = \"summaries_with_embeddings.csv\"\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 5: Generate embeddings and save to a cell\n",
    "Here, we read the input CSV file with summaries, preprocess the summaries, generate embeddings for the preprocessed summaries, and save the resulting DataFrame to a new CSV file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate embeddings and save to a cell\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"preprocessed_summary\"] = df.summary.apply(preprocess_text)\n",
    "df[\"embedding\"] = df.preprocessed_summary.apply(generate_embedding).apply(lambda x: ', '.join(map(str, x[0])))\n",
    "df.to_csv(output_file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 6: Load embeddings and define a search function\n",
    "In this cell, we load the CSV file containing the summaries with embeddings, convert the embeddings column to NumPy arrays, and define a `search` function that wraps the `search_authors_and_relevant_parts` function for easier use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load embeddings and define a search function\n",
    "df_with_embeddings = pd.read_csv(output_file_path)\n",
    "df_with_embeddings[\"embedding\"] = df_with_embeddings.embedding.apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "def search(query):\n",
    "    search_authors_and_relevant_parts(df_with_embeddings, query, n=3, k=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cell 7: Test the search function\n",
    "This cell runs the `search` function with an example query (\"Plant biology\") to see how it works and to display the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Lee, H. (similarity: 0.3164)\n",
      "Relevant parts:  The plants were grown for a period of six weeks, and growth parameters such as shoot and root length, leaf area, and dry weight were measured at regular intervals.The objective of this project was to investigate the effects of different light spectra on the growth and nutritional content of lettuce plants...\n",
      "Author: Smith, J. (similarity: 0.3009)\n",
      "Relevant parts:  The plants were grown under controlled conditions for a period of 10 weeks, and the growth parameters such as plant height, stem diameter, and number of leaves were measured at regular intervals.The aim of this project was to investigate the effect of different types of soil on the growth of tomato plants...\n",
      "Author: Johnson, A. (similarity: 0.2969)\n",
      "Relevant parts:  Moreover, the higher the caffeine concentration, the slower the growth rate of the plants. These findings suggest that caffeine may have a negative impact on the growth of Arabidopsis thaliana and could potentially affect the growth of other plant species...\n"
     ]
    }
   ],
   "source": [
    "# Test the search function\n",
    "\n",
    "search(\"plant biology\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
